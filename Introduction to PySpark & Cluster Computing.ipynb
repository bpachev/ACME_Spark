{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Intro to Cluster Computing</b></h1><br/>\n",
    "Cluster computing, or Grid computing, is all about getting a set of computers to work together and to act like a single system. This system then has each node focused on performing the same task, which can make solving certain tasks much more feasible and efficient, particularly in comparison with the speed or availability of a single computer. Cluster computing has a wide range of applications from small businesses clusters with only a few nodes to some of the fastest supercomputers in the world, like IBM's Sequoia. At the same time, since cluster computing requires software to be purpose-built per task, it is limited in the sense that it is not applicable in casual computing situations. In this lab, we will introduce Apache Hadoop and Apache Spark, which are open source cluster computing frameworks, and will provide some interesting applications of cluster computing in these settings.\n",
    "<h1><b>Apache Hadoop</b></h1><br/>\n",
    "Apache Hadoop is a framework built for the purpose of distributed processing of very large data sets on computer clusters built from commodity hardware. The core of Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part called MapReduce, which you will become familiar with throughout this lab. Hadoop splits files into large blocks and distributes them across nodes in a cluster, so each computer in the cluster can do a fraction of the work, improving efficiency. However, the MapReduce cluster computing paradigm forces a particular linear dataflow structure on distributed programes, which limits the effectiveness of Hadoop, leading to the creation of various modules to overcome these limitations. Particularly, Apache Spark was founded in this endeavor, and is the current framework where much of Hadoop is used. In fact, many would say that Hadoop currently exists only to facilitate the use of Spark in applications.\n",
    "<h1><b>Apache Spark</b></h1><br/>\n",
    "Specifically, Apache Spark is (like Hadoop) an open source cluster computing framework, while at the same time being a module of Hadoop. Spark needs to be installed on top of Hadoop, and is part of Hadoop's \"ecosystem.\" While in Hadoop alone, MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk, Spark's RDDs function as a working set for distrubuted programs that offers a (deliberately) restricted form of distributed shared memory. Actually, the avaiability of RDDs (which will be defined shortly) facilitates the implementation of both iterative algorithms and data analysis, making it much more versatile in applications. In fact, the latency of such applications (compared to Hadoop alone) may be reduced by several orders of magnitude! Particularly, Spark specializes in being used to train algorithms for machine learning systems, and was one of the main inspirations for its creation. Spark's usefulness in machine learning is our motivation for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Spark program first requires instantiating a SparkContext, which represents our connection to the cluster. The first argumement is the URL to the cluster (it will be `\"local\"` for all our examples) and the second arguement is merely a name for our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"My App\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It requires significant setup to run Spark from an ipython notebook. However, if Spark is installed, we can easily run an application on a temporary local cluster using `spark-submit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark-submit --master local[4] sparkApp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This spins up a master spark node with 4 local processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Introduction to RDD's.</b></h1><br/>\n",
    "An RDD is short for Resilient Distributed Dataset. We can create an RDD with Spark, and it will be broken up into chunks and stored on multiple devices. We can then perform parallelized operations on RDD's almost as if it were a single, local, file on one device, without having to worry about the details of multi-processing. This is the power of Spark. This results in very elegant, intuitive source code.\n",
    "<br/><br/>\n",
    "Spark can easily work with files distributed across several nodes in a network using the Hadoop Distributed File System. However, in this lab we will only be using simple local text files.\n",
    "\n",
    "For example, we can create a simple RDD with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "data = sc.textFile(\"example.txt\")\n",
    "\"\"\"example.txt:\n",
    "hello\n",
    "this is a text file\n",
    "another line\n",
    "more lines\n",
    "\"\"\"\n",
    "print data.collect() #Show that data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The function `SparkContext.textFile` creates an RDD from a text file, which atomatically splits it by line into an RDD of strings. Printing `data.collect()` reveals the RDD `data` to be a series of unicode strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Problem 1: Create an RDD from the textfile nums.txt, and print it, following the example code above. Save your code in a file nums.py, as you will be performing further operations on the dataset nums.txt.\n",
    "\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "data = sc.textFile(\"nums.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many basic operations that can be performed on RDDs. A few useful ones include `RDD.collect()`, which returns a list of all elements in the RDD; `RDD.count()`, which gives the number of elements; and `RDD.take(x)` will return the first `x` elements of the RDD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.reduce(lambda a,b:float(a)+float(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "will add up all elements in the data set (first converting them to float), giving the sum of all elements in the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2: RDD Actions.\n",
    "Modify the nums.py file you created in Problem 1 to print the following statistics about the dataset.<br/>\n",
    "(i) Number of lines in the RDD. <br/>\n",
    "(ii) The first 5 elements of the dataset.<br/>\n",
    "(iii) The lexicographically first string in the dataset. (HINT: use reduce with max)<br/>\n",
    "(iv) The entire dataset, as an array.<br/>\n",
    "<br/>\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "data = sc.textFile(\"nums.txt\")\n",
    "print \"Num Lines: %d\" % data.count()\n",
    "print data.take(5)\n",
    "print data.reduce(max)\n",
    "print data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core functionality of Spark is the MapReduce algorithm, which is used for massively parallelized . In the MapReduce algorithm a function is first *mapped* to many nodes. In Spark this is done using `RDD.map(f)`, which also accepts a function `f` to apply to each element of the RDD.\n",
    "\n",
    "Since in spark the data distributed among processors, the it can be processed in parallel. Eventually however, we often need to recombine this data into a single output, for example a sum, maximum, or average. This is done by *reducing* the data. In Spark we use `RDD.reduce(f)` we  two variables with one output and applies this function in a tree-like fashion to all elements in parallel. \n",
    "\n",
    "For example, the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdata = data.map(float)\n",
    "fdata.reduce(lambda a,b:float(a)+float(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will map the function `float` to all elements of the RDD then sum up all elements in parallel, working up in a tree.\n",
    "\n",
    "Another function that can be useful is the `RDD.filter` function, which allows you to filter the data set based on a boolean function. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print fData.filter(lambda a: a < 50.0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "will first filter only the elements under fifty, then give us the number of those elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3: RDD transformations<br/>\n",
    "Further extend your file, nums.py to answer the following questions about your dataset, using RDD transformations (filter, reduce, map).<br/>\n",
    "(i) The number of even numbers (Hint: what map function will put the number in a form that is easy to query for evens?).<br/>\n",
    "(ii) The sum of the numbers in the dataset.<br/>\n",
    "(iii) The sum of the squares of the dataset.<br/><br/>\n",
    "Solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from math import log\n",
    "\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "data = sc.textFile(\"nums.txt\")\n",
    "print \"Num Lines: %d\" % data.count()\n",
    "print \"First five elements.\", data.take(5)\n",
    "print \"Maximum Element \" + data.reduce(max)\n",
    "print \"Total dataset \", data.collect()\n",
    "\n",
    "intData = data.map(int)\n",
    "print \"Numbers less than 50: %d\" % intData.filter(lambda a: a).count()\n",
    "print \"Sum %d \" % intData.reduce(lambda a,b : a+b)\n",
    "print \"Sum Of Squares %d\" % intData.map(lambda x: x*x).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDDs\n",
    "Spark also has the ability to create RDDs out of key/value pairs. Using the `reduceByKey` function, we can combine and reduce results from a pair RDD by key.\n",
    "\n",
    "Creating a pair RDD from a normal RDD is simple: we simply take each element from the original RDD and create a tuple key/value pair from each element using the `map` function. For example, if the file `pairs.txt` contains a list of letter-number pairs, we can load this into a pair RDD using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "data = sc.textFile(\"pairs.txt\")\n",
    "\n",
    "def mapper(line):\n",
    "    key,value = line.split()\n",
    "    return key,int(value)\n",
    "pairData = data.map(mapper)\n",
    "print pairData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then query our pair RDD for the max value *per key* using `RDD.reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxes = pairData.reduce(max)\n",
    "print maxes.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that maxes is an RDD with as many elements as there were keys in `pairData`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4: Basic Map-Reduce<br/>\n",
    "Create a new file, words.py. Create an RDD from words.txt. By following the steps below, use MapReduce to compute the number of words start with each letter.<br/>\n",
    "(i) Create a mapper function map that maps a word to a key-value pair where the key is the first letter of the word, and the value is simply 1, as in the preceding example. (TODO write an example)<br/>\n",
    "(ii) Create a reducer function that simply sums two values.<br/>\n",
    "(iii) Using the operations map, reduceByKey, and collect, determine, for each letter, the number of words that start with that letter.<br/><br/>\n",
    "Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "corpus = \"words.txt\" \n",
    "sc = SparkContext(\"local\", \"Simple App\")\n",
    "#Create the RDD\n",
    "data = sc.textFile(corpus).cache()\n",
    "\n",
    "\n",
    "def mapper(w):\n",
    "  return (w[0], 1)\n",
    "\n",
    "def reducer(a,b):\n",
    " return a+b\n",
    "\n",
    "print data.map(mapper).reduceByKey(reducer).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases we need to split each element of an RDD into smaller elements. For example, our original RDD may contain the lines of a text file as it's elements, but we need an RDD with individual words as the elements. This can be accomplished with `map`'s cousin, `flatmap`. `RDD.flatmap` works exactly like map except if the output of the mapping function is a list, each element of the list will be added to the RDD as a separate element. We can convert our RDD of lines into an RDD of words like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "data = sc.textFile(\"text.txt\")\n",
    "\n",
    "words = data.flatMap(lambda line:line.split())\n",
    "print data.count()\n",
    "print words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5: Advanced Map-Reduce with bigrams.<br/>A bigram is a two-character string, like \"aa\" or \"bj\". The purpose of this problem is to determine frequencies of each bigram in the Brown Corpus. Bigram frequencies are used in crytographic attacks against substitution ciphers.<br/> \n",
    "(i) Write a function bigramMapper that takes in a string and returns a list of all two-character strings in the string. For example, for the input \"alpha\" it should return [\"al\", \"lp\", \"ph\", \"ha\"].<br/>\n",
    "(ii) Use your mapper function in conjunction with flatMap to create an RDD with bigrams as keys and 1 as values.<br/>\n",
    "(iii) Use reduceByKey to find the frequencies of each bigram, and then print out the probability of any given bigram occuring. Congratulations! You are now ready to use Spark for evil, brute-force cryptographic purposes!\n",
    "<br/><br/>\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "corpus = \"/home/benjamin/ACME_Spark/words.txt\" \n",
    "sc = SparkContext(\"local\", \"Simple App\")\n",
    "#Create the RDD\n",
    "data = sc.textFile(corpus).cache()\n",
    "\n",
    "def bigramMapper(w):\n",
    "  return [w[i:i+2] for i in xrange(len(w)-1)]\n",
    "\n",
    "bigrams = data.flatMap(bigramMapper)\n",
    "numBigrams = float(bigrams.count())\n",
    "bigram_freqs = bigrams.map(lambda w: (w,1)).reduceByKey(lambda a,b: a+b).collect()\n",
    "\n",
    "for bigram, freq in bigram_freqs:\n",
    "  print \"Probability of bigram \"+bigram+\" \"+str(freq/numBigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
