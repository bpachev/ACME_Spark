{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Intro to Cluster Computing</b></h1><br/>\n",
    "   - What and why\n",
    "<br/><br/>\n",
    "<h1><b>Hadoop</b></h1><br/>\n",
    "   - What and why, as well as limitations\n",
    "<br/><br/>\n",
    "<h1><b>Spark</b></h1><br/>\n",
    "   - How it overcomes Hadoop's limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Introduction to RDD's.</b></h1><br/>\n",
    "An RDD is short for Resilient Distributed Dataset. We can create an RDD with Spark, and it will be broken up into chunks and stored on multiple devices. We can then perform parallelized operations on RDD's almost as if it were a single, local, file on one device, without having to worry about the details of multi-processing. This is the power of Spark. This results in very elegant, intuitive source code.\n",
    "<br/><br/>\n",
    "For example, we can create a simple RDD with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = sc.SparkContext(\"Local\", \"My App\")\n",
    "data = sc.textFile(\"example.txt\")\n",
    "print data.collect() #Show that data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Problem 1: Create an RDD from the textfile nums.txt, and print it, following the example code above. Save your code in a file nums.py, as you will be performing further operations on the dataset nums.txt.\n",
    "\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-8e4d9588c430>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-8e4d9588c430>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    sc = new SparkContext(\"Local\", \"My App\")\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "data = sc.textFile(\"nums.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2: RDD Actions.\n",
    "Modify the nums.py file you created in Problem 1 to print the following statistics about the dataset.<br/>\n",
    "(i) Number of lines in the RDD. <br/>\n",
    "(ii) The first 5 elements of the dataset.<br/>\n",
    "(iii) The lexicographically first string in the dataset. (HINT: use reduce with max)<br/>\n",
    "(iv) The entire dataset, as an array.<br/>\n",
    "<br/>\n",
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "data = sc.textFile(\"nums.txt\")\n",
    "print \"Num Lines: %d\" % data.count()\n",
    "print data.take(5)\n",
    "print data.reduce(max)\n",
    "print data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3: RDD transformations<br/>\n",
    "Further extend your file, nums.py to answer the following questions about your dataset, using RDD transformations (filter, reduce, map).<br/>\n",
    "(i) The number of numbers less than 50.<br/>\n",
    "(ii) The sum of the numbers in the dataset.<br/>\n",
    "(iii) The sum of the squares of the dataset.<br/><br/>\n",
    "Solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from math import log\n",
    "\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "data = sc.textFile(\"nums.txt\")\n",
    "print \"Num Lines: %d\" % data.count()\n",
    "print \"First five elements.\", data.take(5)\n",
    "print \"Maximum Element \" + data.reduce(max)\n",
    "print \"Total dataset \", data.collect()\n",
    "\n",
    "intData = data.map(int)\n",
    "print \"Numbers less than 50: %d\" % intData.filter(lambda a: a < 50).count()\n",
    "print \"Sum %d \" % intData.reduce(lambda a,b : a+b)\n",
    "print \"Sum Of Squares %d\" % intData.map(lambda x: x*x).reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4: Basic Map-Reduce<br/>\n",
    "Create a new file, words.py. Create an RDD from words.txt. By following the steps below, use MapReduce to compute the number of words start with each letter.<br/>\n",
    "(i) Create a mapper function map that maps a word to a key-value pair where the key is the first letter of the word, and the value is simply 1, as in the preceding example. (TODO write an example)<br/>\n",
    "(ii) Create a reducer function that simply sums two values.<br/>\n",
    "(iii) Using the operations map, reduceByKey, and collect, determine, for each letter, the number of words that start with that letter.<br/><br/>\n",
    "Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "corpus = \"words.txt\" \n",
    "sc = SparkContext(\"local\", \"Simple App\")\n",
    "#Create the RDD\n",
    "data = sc.textFile(corpus).cache()\n",
    "\n",
    "\n",
    "def mapper(w):\n",
    "  return (w[0], 1)\n",
    "\n",
    "def reducer(a,b):\n",
    " return a+b\n",
    "\n",
    "print data.map(mapper).reduceByKey(reducer).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5: Advanced Map-Reduce with bigrams.<br/> Extend the words.py file you made to count the number of bigrams in \n",
    "(i)<br/>\n",
    "(ii)<br/>\n",
    "\n",
    "The following program reads in a file of words (words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
